{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LMCADS03</th>\n",
       "      <th>LMCADY</th>\n",
       "      <th>DXY</th>\n",
       "      <th>SPX</th>\n",
       "      <th>BCOM</th>\n",
       "      <th>MXWD</th>\n",
       "      <th>XAU</th>\n",
       "      <th>XAG</th>\n",
       "      <th>LMCADY_acu_5d_log</th>\n",
       "      <th>LMCADY_std_5d_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.014202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.016023</td>\n",
       "      <td>0.016427</td>\n",
       "      <td>0.004791</td>\n",
       "      <td>0.011739</td>\n",
       "      <td>0.010211</td>\n",
       "      <td>0.009828</td>\n",
       "      <td>0.010703</td>\n",
       "      <td>0.019365</td>\n",
       "      <td>0.034657</td>\n",
       "      <td>0.008692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.104003</td>\n",
       "      <td>-0.103580</td>\n",
       "      <td>-0.027263</td>\n",
       "      <td>-0.127652</td>\n",
       "      <td>-0.064023</td>\n",
       "      <td>-0.099967</td>\n",
       "      <td>-0.095121</td>\n",
       "      <td>-0.203851</td>\n",
       "      <td>-0.252004</td>\n",
       "      <td>0.000803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.007485</td>\n",
       "      <td>-0.007702</td>\n",
       "      <td>-0.002649</td>\n",
       "      <td>-0.003930</td>\n",
       "      <td>-0.005347</td>\n",
       "      <td>-0.003789</td>\n",
       "      <td>-0.004888</td>\n",
       "      <td>-0.008032</td>\n",
       "      <td>-0.016593</td>\n",
       "      <td>0.008627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>0.012146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.008706</td>\n",
       "      <td>0.008933</td>\n",
       "      <td>0.002650</td>\n",
       "      <td>0.005488</td>\n",
       "      <td>0.005580</td>\n",
       "      <td>0.004882</td>\n",
       "      <td>0.006018</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.021674</td>\n",
       "      <td>0.017555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.118805</td>\n",
       "      <td>0.117259</td>\n",
       "      <td>0.025199</td>\n",
       "      <td>0.109572</td>\n",
       "      <td>0.056475</td>\n",
       "      <td>0.089019</td>\n",
       "      <td>0.102451</td>\n",
       "      <td>0.131802</td>\n",
       "      <td>0.191786</td>\n",
       "      <td>0.092058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LMCADS03       LMCADY          DXY          SPX         BCOM  \\\n",
       "count  5550.000000  5550.000000  5550.000000  5550.000000  5550.000000   \n",
       "mean      0.000331     0.000331     0.000006     0.000326    -0.000005   \n",
       "std       0.016023     0.016427     0.004791     0.011739     0.010211   \n",
       "min      -0.104003    -0.103580    -0.027263    -0.127652    -0.064023   \n",
       "25%      -0.007485    -0.007702    -0.002649    -0.003930    -0.005347   \n",
       "50%       0.000000     0.000000     0.000000     0.000390     0.000000   \n",
       "75%       0.008706     0.008933     0.002650     0.005488     0.005580   \n",
       "max       0.118805     0.117259     0.025199     0.109572     0.056475   \n",
       "\n",
       "              MXWD          XAU          XAG  LMCADY_acu_5d_log  \\\n",
       "count  5550.000000  5550.000000  5550.000000        5550.000000   \n",
       "mean      0.000258     0.000343     0.000321           0.001644   \n",
       "std       0.009828     0.010703     0.019365           0.034657   \n",
       "min      -0.099967    -0.095121    -0.203851          -0.252004   \n",
       "25%      -0.003789    -0.004888    -0.008032          -0.016593   \n",
       "50%       0.000644     0.000509     0.000791           0.002310   \n",
       "75%       0.004882     0.006018     0.009851           0.021674   \n",
       "max       0.089019     0.102451     0.131802           0.191786   \n",
       "\n",
       "       LMCADY_std_5d_log  \n",
       "count        5550.000000  \n",
       "mean            0.014202  \n",
       "std             0.008692  \n",
       "min             0.000803  \n",
       "25%             0.008627  \n",
       "50%             0.012146  \n",
       "75%             0.017555  \n",
       "max             0.092058  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./input/copper_log_returns_5d_final.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5550, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.55227417, 0.55884327, 0.72437164, ..., 0.6517713 , 0.45227945,\n",
       "        0.63091865],\n",
       "       [0.52567224, 0.53006073, 0.42503138, ..., 0.54897833, 0.55693906,\n",
       "        0.6515204 ],\n",
       "       [0.51527382, 0.5154988 , 0.42642861, ..., 0.62711759, 0.48144791,\n",
       "        0.61189368],\n",
       "       ...,\n",
       "       [0.44600323, 0.44822604, 0.71993335, ..., 0.48498343, 0.44097188,\n",
       "        0.58567092],\n",
       "       [0.45143493, 0.45403385, 0.52509919, ..., 0.5433245 , 0.56421267,\n",
       "        0.65969574],\n",
       "       [0.52193192, 0.52924947, 0.65677493, ..., 0.46569592, 0.42103467,\n",
       "        0.54740291]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(df.drop(['Date', 'LMCADY_std_5d_log', 'LMCADY_acu_5d_log'], axis=1))\n",
    "print(scaled_features.shape)\n",
    "scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearSecuencias(data, n_steps):\n",
    "    X, y = [], []\n",
    "    try:\n",
    "        data = data.values  # Asegurarse de que 'data' es un array de NumPy\n",
    "    except:\n",
    "        pass\n",
    "    for i in range(n_steps, len(data)):\n",
    "        X.append(data[i-n_steps:i, :-2])  # las variables excepto los target\n",
    "        y.append(data[i, -2:])            # los target\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5525, 25, 6), (5525, 2))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = 25  # ventana modificable\n",
    "X, y = crearSecuencias(scaled_features, n_steps)\n",
    "(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.55227417 0.55884327 0.72437164 0.67578672 0.74159531 0.6517713 ]\n",
      " [0.52567224 0.53006073 0.42503138 0.53606691 0.64989338 0.54897833]\n",
      " [0.51527382 0.5154988  0.42642861 0.63179794 0.39756355 0.62711759]\n",
      " [0.44606453 0.44874882 0.63149789 0.51042887 0.50893837 0.4821214 ]\n",
      " [0.51086559 0.51327568 0.38913311 0.478309   0.5652901  0.45140603]\n",
      " [0.51586387 0.52114596 0.5327604  0.61909187 0.64695536 0.6078601 ]\n",
      " [0.41770099 0.41621381 0.38644071 0.53806236 0.41382434 0.54391078]\n",
      " [0.49820187 0.49829303 0.56480515 0.53215019 0.56169979 0.53296103]\n",
      " [0.51686972 0.51875981 0.47076197 0.56261193 0.52738562 0.56611812]\n",
      " [0.49630121 0.4964232  0.54413481 0.4768538  0.61115836 0.47153276]\n",
      " [0.48546653 0.49150519 0.41593264 0.52145604 0.60195686 0.51290021]\n",
      " [0.42530544 0.42670955 0.47422343 0.47860195 0.52504532 0.46285678]\n",
      " [0.52951787 0.53168539 0.50449486 0.53810781 0.5313222  0.50310721]\n",
      " [0.46147725 0.46429241 0.47407852 0.47139088 0.54641322 0.47080182]\n",
      " [0.48796562 0.49131745 0.51966674 0.49389986 0.57844475 0.47015538]\n",
      " [0.47470042 0.47912416 0.44917059 0.58098579 0.49234458 0.56815132]\n",
      " [0.44828534 0.44881722 0.39710854 0.41303936 0.62528049 0.42974366]\n",
      " [0.42154098 0.42014891 0.52734976 0.46943126 0.42264699 0.41460243]\n",
      " [0.45338887 0.4546726  0.56186807 0.59276512 0.58723669 0.56459344]\n",
      " [0.50418415 0.50721337 0.48130576 0.56658753 0.6441601  0.55182436]\n",
      " [0.56935885 0.57505875 0.56569066 0.44067097 0.58876535 0.4900446 ]\n",
      " [0.4991695  0.5053634  0.59421941 0.59309744 0.52818213 0.55217968]\n",
      " [0.48095904 0.48216979 0.51775877 0.56080603 0.52417043 0.57679191]\n",
      " [0.39152352 0.39094383 0.37409942 0.47829957 0.58210459 0.46251656]\n",
      " [0.46939868 0.47103137 0.61747266 0.51513433 0.52871249 0.53384128]]\n",
      "[[0.52567224 0.53006073 0.42503138 0.53606691 0.64989338 0.54897833]\n",
      " [0.51527382 0.5154988  0.42642861 0.63179794 0.39756355 0.62711759]\n",
      " [0.44606453 0.44874882 0.63149789 0.51042887 0.50893837 0.4821214 ]\n",
      " [0.51086559 0.51327568 0.38913311 0.478309   0.5652901  0.45140603]\n",
      " [0.51586387 0.52114596 0.5327604  0.61909187 0.64695536 0.6078601 ]\n",
      " [0.41770099 0.41621381 0.38644071 0.53806236 0.41382434 0.54391078]\n",
      " [0.49820187 0.49829303 0.56480515 0.53215019 0.56169979 0.53296103]\n",
      " [0.51686972 0.51875981 0.47076197 0.56261193 0.52738562 0.56611812]\n",
      " [0.49630121 0.4964232  0.54413481 0.4768538  0.61115836 0.47153276]\n",
      " [0.48546653 0.49150519 0.41593264 0.52145604 0.60195686 0.51290021]\n",
      " [0.42530544 0.42670955 0.47422343 0.47860195 0.52504532 0.46285678]\n",
      " [0.52951787 0.53168539 0.50449486 0.53810781 0.5313222  0.50310721]\n",
      " [0.46147725 0.46429241 0.47407852 0.47139088 0.54641322 0.47080182]\n",
      " [0.48796562 0.49131745 0.51966674 0.49389986 0.57844475 0.47015538]\n",
      " [0.47470042 0.47912416 0.44917059 0.58098579 0.49234458 0.56815132]\n",
      " [0.44828534 0.44881722 0.39710854 0.41303936 0.62528049 0.42974366]\n",
      " [0.42154098 0.42014891 0.52734976 0.46943126 0.42264699 0.41460243]\n",
      " [0.45338887 0.4546726  0.56186807 0.59276512 0.58723669 0.56459344]\n",
      " [0.50418415 0.50721337 0.48130576 0.56658753 0.6441601  0.55182436]\n",
      " [0.56935885 0.57505875 0.56569066 0.44067097 0.58876535 0.4900446 ]\n",
      " [0.4991695  0.5053634  0.59421941 0.59309744 0.52818213 0.55217968]\n",
      " [0.48095904 0.48216979 0.51775877 0.56080603 0.52417043 0.57679191]\n",
      " [0.39152352 0.39094383 0.37409942 0.47829957 0.58210459 0.46251656]\n",
      " [0.46939868 0.47103137 0.61747266 0.51513433 0.52871249 0.53384128]\n",
      " [0.39825926 0.39842735 0.48328787 0.51083608 0.55838844 0.47101807]]\n"
     ]
    }
   ],
   "source": [
    "# verificar que haya secuencia\n",
    "print(X[0])\n",
    "print(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50216753, 0.60677057],\n",
       "       [0.49401487, 0.58090082],\n",
       "       [0.43348774, 0.6024514 ],\n",
       "       ...,\n",
       "       [0.44097188, 0.58567092],\n",
       "       [0.56421267, 0.65969574],\n",
       "       [0.42103467, 0.54740291]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "\n",
    "    Conv1D(filters=128, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(50, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(2)\n",
    "    \n",
    "])\n",
    "\n",
    "initial_learning_rate = 0.0001\n",
    "optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=[rmse])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.4222 - rmse: 1.5437 - val_loss: 0.2795 - val_rmse: 0.5316\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6444 - rmse: 1.2751 - val_loss: 0.2549 - val_rmse: 0.5081\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.3142 - rmse: 1.1395 - val_loss: 0.1810 - val_rmse: 0.4285\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0689 - rmse: 1.0263 - val_loss: 0.1339 - val_rmse: 0.3687\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8809 - rmse: 0.9332 - val_loss: 0.0960 - val_rmse: 0.3120\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.7144 - rmse: 0.8421 - val_loss: 0.0707 - val_rmse: 0.2644\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.5878 - rmse: 0.7627 - val_loss: 0.0529 - val_rmse: 0.2263\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.5206 - rmse: 0.7182 - val_loss: 0.0549 - val_rmse: 0.2320\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4439 - rmse: 0.6622 - val_loss: 0.0670 - val_rmse: 0.2570\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4068 - rmse: 0.6332 - val_loss: 0.0743 - val_rmse: 0.2732\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3467 - rmse: 0.5862 - val_loss: 0.0788 - val_rmse: 0.2818\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3094 - rmse: 0.5549 - val_loss: 0.0809 - val_rmse: 0.2830\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2922 - rmse: 0.5376 - val_loss: 0.0838 - val_rmse: 0.2901\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2562 - rmse: 0.5030 - val_loss: 0.0831 - val_rmse: 0.2862\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2390 - rmse: 0.4877 - val_loss: 0.0756 - val_rmse: 0.2738\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2188 - rmse: 0.4663 - val_loss: 0.0748 - val_rmse: 0.2683\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2044 - rmse: 0.4501 - val_loss: 0.0756 - val_rmse: 0.2705\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1816 - rmse: 0.4246 - val_loss: 0.0725 - val_rmse: 0.2660\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1718 - rmse: 0.4117 - val_loss: 0.0682 - val_rmse: 0.2585\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1678 - rmse: 0.4080 - val_loss: 0.0672 - val_rmse: 0.2566\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1506 - rmse: 0.3865 - val_loss: 0.0662 - val_rmse: 0.2548\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1418 - rmse: 0.3756 - val_loss: 0.0645 - val_rmse: 0.2515\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1328 - rmse: 0.3629 - val_loss: 0.0615 - val_rmse: 0.2487\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1249 - rmse: 0.3524 - val_loss: 0.0612 - val_rmse: 0.2492\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1206 - rmse: 0.3457 - val_loss: 0.0596 - val_rmse: 0.2464\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1119 - rmse: 0.3328 - val_loss: 0.0578 - val_rmse: 0.2445\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1093 - rmse: 0.3288 - val_loss: 0.0569 - val_rmse: 0.2406\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1015 - rmse: 0.3173 - val_loss: 0.0559 - val_rmse: 0.2396\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0976 - rmse: 0.3111 - val_loss: 0.0547 - val_rmse: 0.2358\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0943 - rmse: 0.3069 - val_loss: 0.0511 - val_rmse: 0.2269\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0902 - rmse: 0.2991 - val_loss: 0.0479 - val_rmse: 0.2205\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0832 - rmse: 0.2875 - val_loss: 0.0494 - val_rmse: 0.2221\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0813 - rmse: 0.2837 - val_loss: 0.0475 - val_rmse: 0.2183\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0794 - rmse: 0.2805 - val_loss: 0.0468 - val_rmse: 0.2169\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0722 - rmse: 0.2680 - val_loss: 0.0447 - val_rmse: 0.2118\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0711 - rmse: 0.2659 - val_loss: 0.0418 - val_rmse: 0.2035\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0663 - rmse: 0.2562 - val_loss: 0.0402 - val_rmse: 0.2002\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0653 - rmse: 0.2547 - val_loss: 0.0397 - val_rmse: 0.2022\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0616 - rmse: 0.2471 - val_loss: 0.0384 - val_rmse: 0.1989\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0573 - rmse: 0.2377 - val_loss: 0.0369 - val_rmse: 0.1947\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0563 - rmse: 0.2365 - val_loss: 0.0369 - val_rmse: 0.1935\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0558 - rmse: 0.2351 - val_loss: 0.0370 - val_rmse: 0.1949\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0560 - rmse: 0.2355 - val_loss: 0.0341 - val_rmse: 0.1876\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0508 - rmse: 0.2248 - val_loss: 0.0317 - val_rmse: 0.1818\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0514 - rmse: 0.2261 - val_loss: 0.0313 - val_rmse: 0.1806\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0483 - rmse: 0.2195 - val_loss: 0.0313 - val_rmse: 0.1803\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0474 - rmse: 0.2171 - val_loss: 0.0299 - val_rmse: 0.1755\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0437 - rmse: 0.2086 - val_loss: 0.0294 - val_rmse: 0.1744\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0455 - rmse: 0.2133 - val_loss: 0.0290 - val_rmse: 0.1732\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0419 - rmse: 0.2040 - val_loss: 0.0288 - val_rmse: 0.1725\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, verbose=1, validation_split=0.15,batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0276 - rmse: 0.1652\n",
      "Loss on test data: [0.0276373028755188, 0.16521000862121582]\n"
     ]
    }
   ],
   "source": [
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Loss on test data: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50216753 0.60677057]\n"
     ]
    }
   ],
   "source": [
    "# imprimir solo los primeros elementos de los arreglos dobles\n",
    "\n",
    "print(y_test[0][:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07702583079247924"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
