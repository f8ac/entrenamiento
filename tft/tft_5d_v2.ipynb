{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, feature_columns, target_column):\n",
    "    features = data[feature_columns]\n",
    "    target = data[target_column]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_features, nhead=4):  \n",
    "    class TransformerModel(pl.LightningModule):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.transformer_layer = nn.TransformerEncoderLayer(d_model=input_features, nhead=4, dim_feedforward=128)\n",
    "            self.encoder = nn.TransformerEncoder(self.transformer_layer, num_layers=1)\n",
    "            self.regressor = nn.Linear(input_features, 1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.encoder(x)\n",
    "            x = self.regressor(x)\n",
    "            return x\n",
    "        \n",
    "        def training_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "            y_hat = self(x)\n",
    "            loss = F.mse_loss(y_hat, y)\n",
    "            return loss\n",
    "        \n",
    "        def validation_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "            y_hat = self(x)\n",
    "            loss = F.mse_loss(y_hat, y)\n",
    "            self.log('val_loss', loss)\n",
    "        \n",
    "        def configure_optimizers(self):\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "            return optimizer\n",
    "\n",
    "    return TransformerModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, batch_size=64, max_epochs=50):\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float), torch.tensor(y_train.values, dtype=torch.float))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float), torch.tensor(y_val.values, dtype=torch.float))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=max_epochs)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(y_true, y_pred, n = -1, title=\"Prediction vs Actual Data\", markersize=3):\n",
    "\n",
    "    if n != -1:\n",
    "        n = min(n, len(y_true), len(y_pred))\n",
    "\n",
    "        y_true = y_true[-n:]\n",
    "        y_pred = y_pred[-n:]\n",
    "\n",
    "    plt.figure(figsize=(18, 3))\n",
    "    plt.plot(y_true, label='Actual Values'   , marker='o', linestyle='-' , markersize=markersize)\n",
    "    plt.plot(y_pred, label='Predicted Values', marker='x', linestyle='--', markersize=markersize)\n",
    "\n",
    "    plt.axhline(y=0, color='gray', linestyle='--', linewidth=0.7)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../input/copper_returns_5d_final.csv'\n",
    "feature_columns = ['LMCADS03', 'DXY', 'SPX', 'BCOM', 'MXWD', 'XAU', 'XAG','LMCADY']  # Ajustar según necesidad\n",
    "target_column = 'LMCADY_acu_5d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67278563, -0.53115652, -0.49950823, ...,  0.38457628,\n",
       "         0.29738832,  0.70733539],\n",
       "       [-0.70391371, -0.99542195,  1.18133841, ...,  0.5955472 ,\n",
       "         0.19294894, -0.72102821],\n",
       "       [-0.77451933, -0.7001329 , -0.04033294, ...,  0.45667364,\n",
       "         0.16570186, -0.74776794],\n",
       "       ...,\n",
       "       [-1.39129872,  0.72681914,  1.25583257, ..., -0.96208851,\n",
       "        -0.6041359 , -1.47491238],\n",
       "       [ 0.02913854, -0.54870262,  0.64189792, ...,  0.87614555,\n",
       "         0.20368924,  0.04514321],\n",
       "       [ 0.10049441, -0.4143716 , -0.21433943, ..., -0.36488917,\n",
       "        -0.13786646,  0.13737986]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data(filepath)\n",
    "X_train, X_val, y_train, y_val = prepare_data(data, feature_columns, target_column)\n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gapuj\\miniconda3\\envs\\cafe\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                    | Params\n",
      "--------------------------------------------------------------\n",
      "0 | transformer_layer | TransformerEncoderLayer | 2.5 K \n",
      "1 | encoder           | TransformerEncoder      | 2.5 K \n",
      "2 | regressor         | Linear                  | 9     \n",
      "--------------------------------------------------------------\n",
      "5.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.0 K     Total params\n",
      "0.020     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gapuj\\miniconda3\\envs\\cafe\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gapuj\\miniconda3\\envs\\cafe\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "C:\\Users\\gapuj\\AppData\\Local\\Temp\\ipykernel_26132\\1143287657.py:23: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(y_hat, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gapuj\\miniconda3\\envs\\cafe\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/70 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gapuj\\AppData\\Local\\Temp\\ipykernel_26132\\1143287657.py:17: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(y_hat, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 70/70 [00:00<00:00, 119.29it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gapuj\\AppData\\Local\\Temp\\ipykernel_26132\\1143287657.py:17: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(y_hat, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 70/70 [00:00<00:00, 104.10it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gapuj\\AppData\\Local\\Temp\\ipykernel_26132\\1143287657.py:23: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(y_hat, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 70/70 [00:00<00:00, 161.05it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 70/70 [00:00<00:00, 104.98it/s, v_num=1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (transformer_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=8, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=128, out_features=8, bias=True)\n",
       "    (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=8, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=8, bias=True)\n",
       "        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (regressor): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(X_train.shape[1])\n",
    "train_model(model, X_train, y_train, X_val, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
