{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LMCADS03</th>\n",
       "      <th>LMCADY</th>\n",
       "      <th>DXY</th>\n",
       "      <th>SPX</th>\n",
       "      <th>BCOM</th>\n",
       "      <th>MXWD</th>\n",
       "      <th>XAU</th>\n",
       "      <th>XAG</th>\n",
       "      <th>LMCADY_acu_5d</th>\n",
       "      <th>LMCADY_std_5d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5539.000000</td>\n",
       "      <td>5539.000000</td>\n",
       "      <td>5539.000000</td>\n",
       "      <td>5539.000000</td>\n",
       "      <td>5539.000000</td>\n",
       "      <td>5539.000000</td>\n",
       "      <td>5539.000000</td>\n",
       "      <td>5539.000000</td>\n",
       "      <td>5539.000000</td>\n",
       "      <td>5539.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000185</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000252</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>-0.000205</td>\n",
       "      <td>-0.000274</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>-0.000867</td>\n",
       "      <td>0.014218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.016059</td>\n",
       "      <td>0.016464</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>0.011794</td>\n",
       "      <td>0.010250</td>\n",
       "      <td>0.009880</td>\n",
       "      <td>0.010722</td>\n",
       "      <td>0.019587</td>\n",
       "      <td>0.034782</td>\n",
       "      <td>0.008721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.112019</td>\n",
       "      <td>-0.110645</td>\n",
       "      <td>-0.024921</td>\n",
       "      <td>-0.103782</td>\n",
       "      <td>-0.054910</td>\n",
       "      <td>-0.085172</td>\n",
       "      <td>-0.097378</td>\n",
       "      <td>-0.123485</td>\n",
       "      <td>-0.185825</td>\n",
       "      <td>0.000805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.008649</td>\n",
       "      <td>-0.008880</td>\n",
       "      <td>-0.002635</td>\n",
       "      <td>-0.005469</td>\n",
       "      <td>-0.005554</td>\n",
       "      <td>-0.004878</td>\n",
       "      <td>-0.005978</td>\n",
       "      <td>-0.009770</td>\n",
       "      <td>-0.021102</td>\n",
       "      <td>0.008616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000643</td>\n",
       "      <td>-0.000497</td>\n",
       "      <td>-0.000771</td>\n",
       "      <td>-0.001844</td>\n",
       "      <td>0.012148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.007562</td>\n",
       "      <td>0.007760</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>0.004908</td>\n",
       "      <td>0.008074</td>\n",
       "      <td>0.017065</td>\n",
       "      <td>0.017520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.109603</td>\n",
       "      <td>0.109134</td>\n",
       "      <td>0.027541</td>\n",
       "      <td>0.136158</td>\n",
       "      <td>0.066117</td>\n",
       "      <td>0.105134</td>\n",
       "      <td>0.099792</td>\n",
       "      <td>0.226116</td>\n",
       "      <td>0.259832</td>\n",
       "      <td>0.091981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LMCADS03       LMCADY          DXY          SPX         BCOM  \\\n",
       "count  5539.000000  5539.000000  5539.000000  5539.000000  5539.000000   \n",
       "mean     -0.000185    -0.000178     0.000007    -0.000252     0.000072   \n",
       "std       0.016059     0.016464     0.004793     0.011794     0.010250   \n",
       "min      -0.112019    -0.110645    -0.024921    -0.103782    -0.054910   \n",
       "25%      -0.008649    -0.008880    -0.002635    -0.005469    -0.005554   \n",
       "50%       0.000000     0.000000     0.000000    -0.000390     0.000000   \n",
       "75%       0.007562     0.007760     0.002658     0.003949     0.005375   \n",
       "max       0.109603     0.109134     0.027541     0.136158     0.066117   \n",
       "\n",
       "              MXWD          XAU          XAG  LMCADY_acu_5d  LMCADY_std_5d  \n",
       "count  5539.000000  5539.000000  5539.000000    5539.000000    5539.000000  \n",
       "mean     -0.000205    -0.000274    -0.000106      -0.000867       0.014218  \n",
       "std       0.009880     0.010722     0.019587       0.034782       0.008721  \n",
       "min      -0.085172    -0.097378    -0.123485      -0.185825       0.000805  \n",
       "25%      -0.004878    -0.005978    -0.009770      -0.021102       0.008616  \n",
       "50%      -0.000643    -0.000497    -0.000771      -0.001844       0.012148  \n",
       "75%       0.003807     0.004908     0.008074       0.017065       0.017520  \n",
       "max       0.105134     0.099792     0.226116       0.259832       0.091981  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./input/copper_returns_5d_final.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5539, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.51105054, 0.50820092, 0.46040927, ..., 0.45376117, 0.47765483,\n",
       "        0.37989689],\n",
       "       [0.50214433, 0.50261079, 0.51343716, ..., 0.46015831, 0.47895721,\n",
       "        0.35204686],\n",
       "       [0.54819756, 0.54585012, 0.39836873, ..., 0.46113695, 0.53109618,\n",
       "        0.36137883],\n",
       "       ...,\n",
       "       [0.45696148, 0.45698224, 0.56849407, ..., 0.35097823, 0.49387949,\n",
       "        0.34883688],\n",
       "       [0.44663175, 0.44252277, 0.56989819, ..., 0.42771448, 0.41879554,\n",
       "        0.31110092],\n",
       "       [0.42031426, 0.41407919, 0.27142421, ..., 0.32700141, 0.52319187,\n",
       "        0.33065703]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(df.drop(['Date', 'LMCADY_std_5d', 'LMCADY_acu_5d'], axis=1))\n",
    "print(scaled_features.shape)\n",
    "scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearSecuencias(data, n_steps):\n",
    "    X, y = [], []\n",
    "    try:\n",
    "        data = data.values  # Asegurarse de que 'data' es un array de NumPy\n",
    "    except:\n",
    "        pass\n",
    "    for i in range(n_steps, len(data)):\n",
    "        X.append(data[i-n_steps:i, :-2])  # las variables excepto los target\n",
    "        y.append(data[i, -2:])            # los target\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5514, 25, 6), (5514, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = 25  # ventana modificable\n",
    "X, y = crearSecuencias(scaled_features, n_steps)\n",
    "(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.51105054 0.50820092 0.46040927 0.44423603 0.50432324 0.45376117]\n",
      " [0.50214433 0.50261079 0.51343716 0.44530546 0.40553668 0.46015831]\n",
      " [0.54819756 0.54585012 0.39836873 0.43838686 0.51559425 0.46113695]\n",
      " [0.49410693 0.49374752 0.36140662 0.41908875 0.4375622  0.41213303]\n",
      " [0.52996151 0.5302504  0.55430507 0.39575223 0.48212846 0.41553961]\n",
      " [0.5619997  0.56257651 0.43096597 0.40912256 0.46098249 0.43504885]\n",
      " [0.49701022 0.49452026 0.447426   0.40636799 0.41316342 0.42126043]\n",
      " [0.41368416 0.40945351 0.4621292  0.45972953 0.44443493 0.48198419]\n",
      " [0.52550391 0.52944731 0.36991284 0.44453469 0.44646772 0.46557395]\n",
      " [0.36897736 0.36294137 0.50655397 0.44057081 0.38628919 0.45033045]\n",
      " [0.5033649  0.5028788  0.45836768 0.38636589 0.46335062 0.40053972]\n",
      " [0.46712256 0.46496218 0.44538254 0.43721685 0.42892662 0.46484468]\n",
      " [0.53753126 0.53518463 0.49544358 0.45992226 0.47799601 0.45961024]\n",
      " [0.47228924 0.47030131 0.5769906  0.39002791 0.41787082 0.39867988]\n",
      " [0.46046999 0.45865672 0.55432041 0.4112173  0.40559091 0.41640271]\n",
      " [0.53254996 0.53331262 0.48053853 0.47545354 0.48415132 0.48751366]\n",
      " [0.48484982 0.48196184 0.48053694 0.43751364 0.41654549 0.44714063]\n",
      " [0.5001435  0.49559762 0.53008776 0.39942532 0.40977373 0.4078881 ]\n",
      " [0.48154347 0.47811841 0.44208974 0.41085467 0.44998967 0.4258089 ]\n",
      " [0.51906883 0.51460121 0.44753218 0.43945628 0.45395453 0.46536463]\n",
      " [0.50118998 0.49596693 0.47502953 0.4254353  0.3799691  0.43698282]\n",
      " [0.55954692 0.56519929 0.49522338 0.44837648 0.41999922 0.46146411]\n",
      " [0.51440303 0.5165903  0.47869725 0.43108481 0.54118718 0.44223604]\n",
      " [0.48311093 0.48041781 0.48419707 0.34632147 0.46552444 0.36117227]\n",
      " [0.48326391 0.4859449  0.48785792 0.4272723  0.39213482 0.44979989]]\n",
      "[[0.50214433 0.50261079 0.51343716 0.44530546 0.40553668 0.46015831]\n",
      " [0.54819756 0.54585012 0.39836873 0.43838686 0.51559425 0.46113695]\n",
      " [0.49410693 0.49374752 0.36140662 0.41908875 0.4375622  0.41213303]\n",
      " [0.52996151 0.5302504  0.55430507 0.39575223 0.48212846 0.41553961]\n",
      " [0.5619997  0.56257651 0.43096597 0.40912256 0.46098249 0.43504885]\n",
      " [0.49701022 0.49452026 0.447426   0.40636799 0.41316342 0.42126043]\n",
      " [0.41368416 0.40945351 0.4621292  0.45972953 0.44443493 0.48198419]\n",
      " [0.52550391 0.52944731 0.36991284 0.44453469 0.44646772 0.46557395]\n",
      " [0.36897736 0.36294137 0.50655397 0.44057081 0.38628919 0.45033045]\n",
      " [0.5033649  0.5028788  0.45836768 0.38636589 0.46335062 0.40053972]\n",
      " [0.46712256 0.46496218 0.44538254 0.43721685 0.42892662 0.46484468]\n",
      " [0.53753126 0.53518463 0.49544358 0.45992226 0.47799601 0.45961024]\n",
      " [0.47228924 0.47030131 0.5769906  0.39002791 0.41787082 0.39867988]\n",
      " [0.46046999 0.45865672 0.55432041 0.4112173  0.40559091 0.41640271]\n",
      " [0.53254996 0.53331262 0.48053853 0.47545354 0.48415132 0.48751366]\n",
      " [0.48484982 0.48196184 0.48053694 0.43751364 0.41654549 0.44714063]\n",
      " [0.5001435  0.49559762 0.53008776 0.39942532 0.40977373 0.4078881 ]\n",
      " [0.48154347 0.47811841 0.44208974 0.41085467 0.44998967 0.4258089 ]\n",
      " [0.51906883 0.51460121 0.44753218 0.43945628 0.45395453 0.46536463]\n",
      " [0.50118998 0.49596693 0.47502953 0.4254353  0.3799691  0.43698282]\n",
      " [0.55954692 0.56519929 0.49522338 0.44837648 0.41999922 0.46146411]\n",
      " [0.51440303 0.5165903  0.47869725 0.43108481 0.54118718 0.44223604]\n",
      " [0.48311093 0.48041781 0.48419707 0.34632147 0.46552444 0.36117227]\n",
      " [0.48326391 0.4859449  0.48785792 0.4272723  0.39213482 0.44979989]\n",
      " [0.47067942 0.46536342 0.51348882 0.45771402 0.48982351 0.46426598]]\n"
     ]
    }
   ],
   "source": [
    "# verificar que haya secuencia\n",
    "print(X[0])\n",
    "print(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47584125, 0.35551757],\n",
       "       [0.48477789, 0.40260268],\n",
       "       [0.47073196, 0.29257905],\n",
       "       ...,\n",
       "       [0.49387949, 0.34883688],\n",
       "       [0.41879554, 0.31110092],\n",
       "       [0.52319187, 0.33065703]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "\n",
    "    Conv1D(filters=128, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(50, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(2)\n",
    "    \n",
    "])\n",
    "\n",
    "initial_learning_rate = 0.0001\n",
    "optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=[rmse])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "63/63 [==============================] - 6s 12ms/step - loss: 2.3208 - rmse: 1.5086 - val_loss: 0.1923 - val_rmse: 0.4385\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.5341 - rmse: 1.2275 - val_loss: 0.2559 - val_rmse: 0.5058\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1897 - rmse: 1.0823 - val_loss: 0.3072 - val_rmse: 0.5542\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9688 - rmse: 0.9784 - val_loss: 0.3143 - val_rmse: 0.5606\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.7754 - rmse: 0.8756 - val_loss: 0.2588 - val_rmse: 0.5087\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.6357 - rmse: 0.7941 - val_loss: 0.1934 - val_rmse: 0.4397\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.5576 - rmse: 0.7421 - val_loss: 0.1254 - val_rmse: 0.3540\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4569 - rmse: 0.6736 - val_loss: 0.0937 - val_rmse: 0.3059\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4165 - rmse: 0.6422 - val_loss: 0.0630 - val_rmse: 0.2508\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.3446 - rmse: 0.5841 - val_loss: 0.0454 - val_rmse: 0.2129\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.3170 - rmse: 0.5600 - val_loss: 0.0465 - val_rmse: 0.2154\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2941 - rmse: 0.5400 - val_loss: 0.0435 - val_rmse: 0.2083\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2502 - rmse: 0.5002 - val_loss: 0.0435 - val_rmse: 0.2083\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2363 - rmse: 0.4842 - val_loss: 0.0445 - val_rmse: 0.2108\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.2135 - rmse: 0.4603 - val_loss: 0.0444 - val_rmse: 0.2104\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2030 - rmse: 0.4481 - val_loss: 0.0439 - val_rmse: 0.2093\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1810 - rmse: 0.4232 - val_loss: 0.0446 - val_rmse: 0.2108\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1648 - rmse: 0.4048 - val_loss: 0.0431 - val_rmse: 0.2073\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1535 - rmse: 0.3897 - val_loss: 0.0398 - val_rmse: 0.1992\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1400 - rmse: 0.3739 - val_loss: 0.0379 - val_rmse: 0.1944\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1250 - rmse: 0.3516 - val_loss: 0.0374 - val_rmse: 0.1931\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1254 - rmse: 0.3533 - val_loss: 0.0377 - val_rmse: 0.1940\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1164 - rmse: 0.3390 - val_loss: 0.0374 - val_rmse: 0.1933\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1074 - rmse: 0.3258 - val_loss: 0.0353 - val_rmse: 0.1875\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1062 - rmse: 0.3245 - val_loss: 0.0352 - val_rmse: 0.1872\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.0970 - rmse: 0.3100 - val_loss: 0.0349 - val_rmse: 0.1867\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0912 - rmse: 0.3004 - val_loss: 0.0340 - val_rmse: 0.1842\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.0860 - rmse: 0.2928 - val_loss: 0.0331 - val_rmse: 0.1817\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.0842 - rmse: 0.2888 - val_loss: 0.0336 - val_rmse: 0.1830\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0809 - rmse: 0.2827 - val_loss: 0.0319 - val_rmse: 0.1783\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0753 - rmse: 0.2730 - val_loss: 0.0318 - val_rmse: 0.1781\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0717 - rmse: 0.2672 - val_loss: 0.0316 - val_rmse: 0.1775\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0662 - rmse: 0.2571 - val_loss: 0.0308 - val_rmse: 0.1752\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.0628 - rmse: 0.2489 - val_loss: 0.0300 - val_rmse: 0.1731\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0644 - rmse: 0.2520 - val_loss: 0.0299 - val_rmse: 0.1726\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0591 - rmse: 0.2427 - val_loss: 0.0289 - val_rmse: 0.1699\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0550 - rmse: 0.2331 - val_loss: 0.0280 - val_rmse: 0.1670\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0536 - rmse: 0.2312 - val_loss: 0.0260 - val_rmse: 0.1611\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0518 - rmse: 0.2263 - val_loss: 0.0257 - val_rmse: 0.1602\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0496 - rmse: 0.2222 - val_loss: 0.0248 - val_rmse: 0.1573\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0477 - rmse: 0.2169 - val_loss: 0.0243 - val_rmse: 0.1558\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0452 - rmse: 0.2111 - val_loss: 0.0238 - val_rmse: 0.1540\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0442 - rmse: 0.2088 - val_loss: 0.0232 - val_rmse: 0.1520\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0416 - rmse: 0.2027 - val_loss: 0.0217 - val_rmse: 0.1471\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0405 - rmse: 0.1999 - val_loss: 0.0204 - val_rmse: 0.1428\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0389 - rmse: 0.1957 - val_loss: 0.0200 - val_rmse: 0.1411\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0362 - rmse: 0.1886 - val_loss: 0.0207 - val_rmse: 0.1436\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0364 - rmse: 0.1898 - val_loss: 0.0195 - val_rmse: 0.1396\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0339 - rmse: 0.1835 - val_loss: 0.0185 - val_rmse: 0.1357\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0326 - rmse: 0.1797 - val_loss: 0.0174 - val_rmse: 0.1319\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, verbose=1, validation_split=0.15,batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0176 - rmse: 0.1321\n",
      "Loss on test data: [0.017575090751051903, 0.132104754447937]\n"
     ]
    }
   ],
   "source": [
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Loss on test data: {loss}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
